\documentclass[11pt]{article}
\usepackage{mathptmx}
\usepackage[left=2.8cm,right=2.8cm]{geometry}
\usepackage{graphicx}
\usepackage{float}
\usepackage{microtype}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\lstset{
	%language=C,
	basicstyle=\small\ttfamily,
	showspaces=false,
	showstringspaces=false,
	showtabs=false,
	tabsize=4,
	breaklines=true,
	breakatwhitespace=false,
	%frame=single,
	%title=\textbf{Source Code: \lstname},
	%numbers=left,
	%numberstyle=\normalsize,
	keywordstyle=\color[rgb]{0,0,1},
	commentstyle=\color[rgb]{0.133,0.545,0.133},
	stringstyle=\color[rgb]{0.627,0.126,0.941},
}

\begin{document}

\begin{center}
{\LARGE Implementation of an optimizing compiler for a simple programming
language using an LLVM back-end} \\
\vspace{8mm}
{\Large Independent Project} \\
\vspace{4mm}
{\large Eric Biggers}
\end{center}
\vspace{5mm}

\begin{abstract}
    Designing a retargetable optimizing compiler is a very difficult task.
    Modern optimizing compilers such as GCC are extremely complex pieces of
    software that support hundreds of different optimizations and many different
    target architectures.  In contrast to the GCC project, the LLVM project
    attempts to encapsulate its compiler infrastructure in libraries that can be
    reused by external programs.  Here I present a compiler for a toy
    programming language called ``garter'' which takes advantage of the LLVM
    infrastructure to implement an optimizing compiler which performs
    state-of-the-art optimizations on the generated code.  I also present an
    interpreter for garter that uses LLVM to perform just-in-time (JIT)
    compilation.  In addition to showing the usefulness of LLVM, this project
    serves as a demonstration of the construction of a compiler front-end.
\end{abstract}

\tableofcontents

\section{Introduction}

A compiler is a program that translates source code written in a
programming language to another computer language, often assembly language for a
physical computer.  The standard software architecture of such a program can be
divided into the front-end, middle-end, and back-end.  The front-end consists of
a lexer, which produces a sequence of syntactical tokens from a string in the
input language, and the parser, which interprets the sequence of syntactical
tokens as a string in the programming language (specified with a formal
context-free grammar) and builds the corresponding abstract syntax tree (AST). \
The middle-end consists of various language-dependent semantic analysis passes
to elaborate the meaning of the program as well as various optimization passes
to optimize the AST. \ The back-end consists of the translation of the AST into
the target language, also known as code generation.

Traditional compilers, such as GCC, reuse code in two primary ways.  First, they
implement support for multiple programming languages by using different
front-ends but shared back-ends.  Second, they implement support for multiple
target machines by using different back-ends.  Many optimization passes in the
middle-end can also be shared between all front-ends and back-ends, although
semantic analysis may be coupled to specific front-ends, and some optimization passes may
be coupled to specific back-ends.

Still, despite the code reuse possible in the GCC project, it is primarily
intended to be a monolithic compiler and is not readily accessible to outside
projects.  Projects such as CodeViz \cite{codeviz} that have attempted to reuse
GCC code simply patch an entire GCC distribution, creating an ugly,
difficult-to-maintain design.  The {\em sparse} project \cite{sparse}, created
by Linus Torvalds to semantically analyze Linux kernel code for programming
errors, implemented its own C front-end rather than reuse GCC's, which the
sparse FAQ criticized as complex and difficult to reuse due to its tight
integration into GCC.  In addition, GCC does not export any interfaces for
just-in-time (JIT) code generation, nor does it have a standard, well-defined
intermediate representation.

In contrast to GCC, the LLVM\footnote{Originally ``Low Level Virtual Machine'',
the acronym has since been dropped because the project's scope is a compiler
infrastructure that supports a broad range of applications, not only virtual
machines.} project provides a compiler infrastructure consisting of a
well-defined language-agnostic intermediate representation and a set of reusable
libraries, including libraries for optimization and code generation
\cite{lattner2004llvm}.  Originally designed for {\em life-long}
(including install-time and runtime) optimization of programs in contrast to
traditional compilers, due to its reusable design LLVM has also been proven
useful to use as the back-end for traditional static compilers.  Although the
most developed compiler using an LLVM back-end is the clang project which
supports C, C++, Objective C, and Objective C++, perhaps the main strength of
LLVM comes from the ease of using it as a back-end for more esoteric or
experimental programming languages, such as CUDA \cite{nvidia_llvm} or Rust
\cite{rust}.  The LLVM authors themselves have even provided a tutorial for
implementing a JIT compiler for a simple programming language using LLVM
\cite{kaleidoscope}.

In this report I present a compiler for a toy programming language called
``garter'' which takes advantage of the LLVM infrastructure to implement an
optimizing compiler which performs state-of-the-art optimizations on the
generated code.  I also present an interpreter for garter that uses LLVM to
perform just-in-time (JIT) compilation.  The garter language includes
fundamental programming language concepts such as mutable variables, loops, and
functions, although it does not support any types except signed 32-bit integers.
The compiler is an ahead-of-time compiler that generates native object files or
executables, whereas the interpreter executes garter code statement-by-statement
using either interpretation or JIT compilation, depending on the method the LLVM
execution engine chooses.  This project also serves as a demonstration of the
construction of a simple compiler front-end, including a lexer, a parser, and an
AST design.  Finally, this project also serves as a demonstration of some of the
issues that arise in practical compiler construction that may get glossed over
in a purely theoretical treatment of the subject.

The source code for this project may be found online at
\url{https://github.com/ebiggers/garter}.  It can be downloaded using git with
the following command:

\begin{verbatim}
$ git clone https://github.com/ebiggers/garter
\end{verbatim}

\section{The garter language}

The garter language implemented for this project is based on a document by Susan
Fox specifying its formal grammar \cite{garter}, which will not be replicated
here.  The garter language can be described as a simple Python-like
language.  However, unlike Python, the semantics of a garter program are
sufficiently static that it can easily be compiled ahead-of-time, like C, rather
than interpreted.

I did make several modifications to the garter language, noted below:

\begin{itemize}
    \item Lists are not implemented.  Although the {\tt in}, {\tt `['}, and {\tt
        `]'} tokens are recognized by the lexer, they are not used in any
        production by the parser.
    \item The {\tt True} and {\tt False} keywords are not implemented.  They
        could simply be mapped to 1 and 0, respectively, but this did not make
        much sense because the language does not yet have an actual boolean type.
    \item I implemented {\tt continue} and {\tt break} statements with the same
        semantics as in C.
    \item I implemented an {\tt extern} keyword, which when followed by a
        function definition beginning with {\tt def} defines a function with
        external linkage rather than internal linkage.  (This is not quite the
        same as C, which uses external linkage by default and requires the {\tt
        static} keyword to specify internal linkage.)
\end{itemize}

\section{Frontend}

This section presents the design and implementation of the garter front-end,
which consists of the lexer and parser.  As a whole, the front-end takes as input
a program in the garter language and produces as output either the resulting
AST (abstract syntax tree) for the full program, or a series of ASTs for the
top-level items (statements or function definitions) in the program.

The front-end is implemented in C++ and is located in the {\tt frontend/}
directory.  I originally planned to implement the front-end using the {\em flex}
and {\em bison} tools, which automatically generate scanners and parsers,
respectively, given specifications for the language's tokens and formal grammar,
respectively.  Although in theory these tools greatly simplify the task of
implementing a compiler front-end, and I have used them before, the complexity
of using them rapidly rises if a high-quality implementation is desired.  For
example, various hacks need to be used if reentrant code or descriptive parser
error messages are desired.  Because of this, many real-world compilers use
hand-coded lexers and parsers rather than automatically generated ones, and I
decided to follow the same route.  My design uses C++ classes and does not have
any global variables, unlike typical designs with {\em flex} and {\em bison}.

\subsection{Lexer}

\label{sec:Lexer}

The garter lexer is implemented by the class {\tt garter::Lexer}.  A Lexer can
be constructed by passing a reference to an input stream ({\tt std::istream}) or
string ({\tt const char *}) from which the garter language input will be read.  The method {\tt
garter::Lexer::getNextToken()} can then be called repeatedly to tokenize the
input into {\tt garter::Token}'s, which describe the various syntactical
elements such as keywords, identifiers, numbers, and operators.  Special error
and end-of-file tokens are defined.  {\tt getNextToken()} returns the error
token as soon as the lexer detects that the input is not a well-formed garter
program.  The lexer also prints an appropriate English error message to standard
error when such an error occurs.

The lexer is implemented as a simple predictive lexer.  {\tt getNextToken()}
reads as many characters as it needs to determine the type of the next token and
pass control to the appropriate block of code.  Identifiers and keywords are
recognized by the same procedure, with keywords being distinguished by their
membership in a pre-initialized set.

The lexer rejects number literals that do not fit into a 32-bit signed integer.

Unit tests for the lexer are described in Section \ref{sec:testing}.

\subsection{Parser}

\label{sec:Parser}

The garter parser is implemented by the class {\tt garter::Parser}.  Like the
Lexer, a Parser can be constructed by passing a reference to either an input
stream or a string.  The parser automatically creates a Lexer internally.
Consequently, a typical user of the front-end need only create the Parser.  The
parser translates the source program into either a single AST for the entire
program (if {\tt garter::Parser::parseProgram()} is called) or a sequence of
ASTs for each top-level statement or function definition (if \\{\tt
garter::Parser::parseTopLevelItem()} is called repeatedly).
%The former interface is used
%by the compiler, whereas the latter interface is used by the interpreter.

The parser is implemented as a relatively straightforward recursive-descent
parser.  It has one procedure for each production in the formal grammar.  Each
such procedure returns an appropriate AST for the production, or a null pointer
on error.  The parser looks ahead however many tokens are necessary to
disambiguate the possible productions at a given point.  Figure
\ref{fig:parseStatement} shows an example of one such parsing procedure.

\begin{figure}
\lstset{language=C++}
\begin{lstlisting}
std::unique_ptr<StatementAST>
Parser::parseStatement()
{
	switch (CurrentToken->getType()) {
	case Token::Break:
		return parseBreakStatement();
	case Token::Continue:
		return parseContinueStatement();
	case Token::Identifier:
		nextTokenLookahead();
		if (NextToken->getType() == Token::Equals)
			return parseAssignmentStatement();
		else
			return parseExpressionStatement();
	case Token::If:
		return parseIfStatement();
	case Token::Pass:
		return parsePassStatement();
	case Token::Print:
		return parsePrintStatement();
	case Token::Return:
		return parseReturnStatement();
	case Token::While:
		return parseWhileStatement();
	default:
		return parseExpressionStatement();
	}
}
\end{lstlisting}
\caption{A code sample from the Parser class.  This procedure is responsible for
parsing a statement.  Since there are many types of statements allowed in the
garter language, this procedure uses the next token(s) to determine the specific
type of statement and call the appropriate parsing procedure.  }
\label{fig:parseStatement}
\end{figure}

The AST types are all declared in {\tt Parser.h} and follow a fairly standard
design.  All types of statements derive from {\tt StatementAST}, and all types
of expressions derive from {\tt ExpressionAST}.  Each AST type has the members
needed to specify the corresponding source construct. For example, a {\tt
WhileStatementAST} contains an {\tt ExpressionAST} that specifies the while loop
condition and an array of {\tt StatementAST}'s that specify the while loop body.
The implementation is complicated slightly by code that I added to print a
string representation of each AST type and also support visitors that operate on each
node in an AST.

If a parse error occurs, the parser or lexer prints an appropriate English error
message to standard error.

Unit tests for the parser are described in Section \ref{sec:testing}.

\section{LLVM back-end}

The garter LLVM back-end, implemented by the class {\tt garter::LLVMBackend}, is
responsible for translating the garter AST into the LLVM IR (intermediate
representation), then optimizing the IR and generating native code.

\subsection{Translation to LLVM IR}

The majority of the code in LLVMBackend is involved in translation from the
garter AST to LLVM IR.  Whereas the garter AST is a close-to-source
representation of garter programs specific to my implementation, LLVM IR is a
fairly low-level, language-agnostic language defined by the LLVM project.
Although mainly used by LLVM internally, LLVM IR is a well-defined language can
be represented in equivalent text, binary, and in-memory forms.  It resembles an
abstract RISC-like instruction set, but it includes key higher-level
information, such as type information, explicit control flow graphs, and an
explicit dataflow representation.  This higher-level information is useful for
code analysis performed during optimization and code generation
\cite{lattner2004llvm}.

The constructor for an LLVMBackend creates a unique per-thread LLVM context ({\tt
llvm::LLVMContext}) and an unnamed LLVM module ({\tt llvm::Module}).
A LLVM module is a top-level container for LLVM IR objects. The {\tt
LLVMBackend::generateProgramIR()} method
then fills this LLVM module with the generated IR.

{\tt generateProgramIR()} begins with a pass
that collects and generates prototypes for all defined functions.
Each such prototype is generated by creating a named and typed {\tt
llvm::Function} object in the module.  {\tt generateProgramIR()} then generates a
prototype for {\tt main()}, which will contain the top-level statements in the
program and will be the entry point of the program.\footnote{
    {\tt main()} is expected to be called automatically when the program is
    executed. {\tt garterc} implements this by linking with the startup code
that normally calls {\tt main()} in C programs.}

Next, {\tt generateProgramIR()} generates LLVM
IR for all function definitions, including {\tt main()}.
For each function, {\tt generateProgramIR()} first creates a basic block ({\tt llvm::BasicBlock}) at
the beginning of the function's LLVM IR representation and creates LLVM IR
instructions that store the function's parameters on the stack.\footnote{Values
    in the LLVM IR are immutable; however, mutable variables can be simulated by
assigning them to stack slots.  LLVM contains an optimization pass that promotes
such variables to immutable LLVM IR values when possible \cite{kaleidoscope}.}
{\tt generateProgramIR()} then generates LLVM IR for each statement in the function and
finally finishes with a return statement in case the garter program did not
explicitly include one at the end of the function definition.

As noted, LLVM IR denotes explicit basic blocks and does not have high-level
control flow statements.  Consequently, control flow statements such as {\tt
while} statements must be translated into basic blocks.

The LLVM IR relies heavily on the class {\tt llvm::Value}, which represents a
typed value computed by the program that may be used as an operand to another
value.  Examples of LLVM values are constants, number variables, and pointer
variables.  The LLVMBackend uses a {\tt NamedValues} map to keep track of which
variables are declared within each function.  When the program being compiled
references a variable, the LLVMBackend looks up its name in the {\tt
NamedValues} map to translate it into the corresponding {\tt llvm::Value}.
Alternatively, when the program being compiled assigns to a variable for the
first time, the LLVMBackend creates an {\tt llvm::Value} for it (through a stack
allocation) and saves a reference to it in the {\tt NamedValues} map.

For generation of arithmetic operations and tests, LLVM provides functions that
take in two values and create the specified arithmetic operation.  For example,
{\tt llvm::IRBuilder::CreateAdd()} takes in two {\tt llvm::Value}'s and returns
a new {\tt llvm::Value} that is the result of a typed arithmetic addition.

For {\tt print} statements, the back-end generates calls to {\tt
\_\_garter\_print()}, and for exponentiation (via the {\tt **} operator), the
back-end generates calls to {\tt \_\_garter\_exponentiate()}.  Both these
functions are
expected to be available at runtime; more details can be found in Section
\ref{sec:runtime}.

\subsection{Optimization}

LLVM provides a modular infrastructure for {\em passes}, which typically perform
some sort of analysis or optimizing transformation on the IR contained in an
LLVM module.  Passes can be run on an LLVM module by populating an instance of
{\tt llvm::PassManager} with passes, then calling {\tt
llvm::PassManager::run()}.  Dozens of different optimization passes are
available, including but not limited to dead code elimination, constant propagation, instruction
combining, global variable optimization, function inlining, loop strength
reduction, loop unrolling, tail call elimination, loop invariant code motion,
and automatic vectorization.

Although LLVM users can choose the specific analysis and optimization passes to
run on the IR, for basic use the {\tt llvm::PassManagerBuilder} helper class can
populate the PassManager with default passes for a specified numeric
optimization level.  I use the latter approach in my compiler, although future
work could certainly look into the usefulness of specific optimizations or even
implement custom LLVM optimization passes.

\subsection{Code generation}

Once an LLVM module has been constructed and optionally optimized, LLVM can
either output it in LLVM IR (assembly or bitcode) or perform code generation to
translate it into an object file containing machine code.\footnote{Through the
    MC sub-project \cite{mc}, LLVM now contains an integrated assembler for most
architectures.}  Unlike the case with GCC, a single LLVM installation can
support multiple target architectures.  However, for the garter implementation,
I currently only have the LLVMBackend request the native target.  After a
pointer to the native {\tt llvm::TargetMachine} is obtained, the LLVMBackend
uses it to schedule an object file generation pass that generates a native
object file.

Alternatively, LLVMBackend also exports a method {\tt
LLVMBackend::executeTopLevelItem()} that is used by the garter interpreter.  It
is intended to be called repeatedly for each top-level item (statement or
function) in a garter program.  If passed the AST for a function definition,
{\tt executeTopLevelItem()} simply compiles the
function definition to LLVM IR.  But if passed a statement
AST, {\tt executeTopLevelItem()} creates an anonymous LLVM IR function
containing the statement, then executes it using an instance of {\tt
llvm::ExecutionEngine}.  An ExecutionEngine either interprets the LLVM IR, or
JIT compiles it and executes it.  If the statement calls any functions defined
in the module, the ExecutionEngine interprets or JIT compiles them as needed.
All in all, this behavior makes possible a working interpreter for
the garter language with little new code over that already used to implement the
ahead-of-time compiler.

\section{Runtime library}

\label{sec:runtime}

Currently, {\tt print} statements and the exponentiation operator ({\tt **}) require
runtime support.  {\tt print} statements are implemented using the C++ function
shown in Figure \ref{fig:print}, whereas exponentiation is implemented using the
garter function shown in Figure \ref{fig:exponentiate}.  In both cases, the
garter compiler and interpreter generate calls to the corresponding function to
implement each language construct.  In the case of the compiler, the object
files containing the runtime functions are linked into the final executable when
the linker is run.  In the case of the interpreter, the object files containing
the runtime functions are linked into the interpreter itself and therefore are
available at execution time.

A future ``improvement'' could be to directly generate LLVM IR for
exponentiation in order to allow optimizations that take advantage of
constraints on the operands, such as constant folding and/or the use of less
costly operations such as bitshifts.

\begin{figure}
    \lstset{language=C++}
    \lstset{numbers=left}
    \lstinputlisting{../runtime/print.cc}
    \caption{Runtime support for the {\tt print} statement.}
    \label{fig:print}
\end{figure}

\begin{figure}
    \lstset{numbers=left}
    \lstinputlisting{../runtime/exponentiate.ga}
    \caption{Implementation of garter's exponentiation operator in garter itself.
        The {\tt extern} keyword is provided to cause the {\tt
        \_\_garter\_exponentiate()} symbol to be given external linkage.  Use of
        the {\tt **} operator {\em within} {\tt \_\_garter\_exponentiate()} is
        indeed valid, and it generates a recursive call; this is used to
        implement the recursive exponentiation by squaring algorithm being used.
    }
    \label{fig:exponentiate}
\end{figure}

\section{Compiler and interpreter}

After having implemented the front-end and back-end, I implemented the actual
{\tt garterc} and {\tt garteri} programs, which provide the ahead-of-time
compiler and interpreter, respectively.

Since it is quite short, the main function for {\tt garteri} is shown in Figure
\ref{fig:garteri}.  It simply sets up a stream to read from, then sets up a
Parser on it that reads individual statements or function definitions from it,
each of which is sent to the LLVMBackend for lowering and, in the case of
statements, execution.

{\tt garterc} is slightly more complicated.  It mirrors the standard behavior of
compilers such as GCC by accepting the {\tt -o} option, which specifies the
location of the output file, and the {\tt -c} option, which indicates to skip
linking and leave the output as an unlinked object file.  In addition, I
implemented a {\tt -l} option, which causes the output to be LLVM IR rather than
native code.  Many additional command-line arguments are also accepted; they
control various aspects of code generation and were added automatically by
linking with the LLVM command-line parsing library.  {\tt garterc} performs
linking by calling {\tt clang} rather than {\tt ld} itself; this is done because
the locations of various libraries and object files (such as the C runtime
startup objects that call {\tt main()}) are system-specific and cannot easily be
determined.\footnote{The Rust compiler\cite{rust} actually ran into the same
    problem and also uses {\tt clang} for linking.}

Figure \ref{fig:test} shows an example of the optimized LLVM IR that {\tt
garterc} produces from an input file.  Although it is a simple example, it shows
that {\tt garterc} benefits from the optimization passes implemented in LLVM,
which are the same state-of-the-art optimizations used by production quality
compilers such as {\tt clang} that rely on LLVM.

\begin{figure}
    \lstset{language=C++}
    \lstset{numbers=left}
    \lstinputlisting{../garteri.cc}
    \caption{{\tt garteri.cc}:  the interpreter for the Garter language.  It
        works, but it is a fairly minimal wrapper around the front-end and
    back-end.}
    \label{fig:garteri}.
\end{figure}

\begin{figure}
    \begin{tabular}{p{7cm}p{9cm}}
        \lstinputlisting{test.ga}
        &
        \lstinputlisting{test.ll}
    \end{tabular}
    \caption{garter functions translated into LLVM IR and optimized using the
            default level 2 optimizations.  Original source is shown on left;
            generated LLVM IR (in assembly format) is shown on right.  LLVM
            optimized {\tt is\_odd()} to a simple bitwise and.  LLVM optimized
            {\tt is\_even()} by inlining the call to {\tt is\_odd()} and
            inverting the result.  LLVM optimized {\tt mul8} in several ways.
            Constant folding simplified {\tt 100 - 100} to 0, which the
            instruction combining pass then removed from the addition
            expression.  Constant folding simplified {\tt n * 2 * 2 * 2} to {\tt
            n * 8}, and instruction combining replaced {\tt n * 8} with a left
            shift by 3 bits.
    }
    \label{fig:test}
\end{figure}

\section{Testing}

\label{sec:testing}

I verified the proper functioning of the compiler and interpreter through
automated testing.

First, I implemented unit test cases for the Lexer (Section \ref{sec:Lexer}) to
verify its proper operation with various short input strings.  Each such test
case specifies an input string and the expected output tokens which must be
produced by the Lexer to pass the test.  An example test case is shown in Figure
\ref{fig:TestLexer}.

\begin{figure}
    \lstset{language=C++}
    \begin{lstlisting}
        {
            .Input = "-10**2 + 3/b",
            .ExpectedOutput =
            {
                ExpectedToken(Token::Minus),
                ExpectedToken(10),
                ExpectedToken(Token::DoubleAsterisk),
                ExpectedToken(2),
                ExpectedToken(Token::Plus),
                ExpectedToken(3),
                ExpectedToken(Token::ForwardSlash),
                ExpectedToken("b"),
            },
        },
    \end{lstlisting}
    \caption{Sample test case for the garter lexer that verifies its ability to
        correctly scan several number literals, operators, and an identifier.}
    \label{fig:TestLexer}
\end{figure}

Second, I implemented unit test cases for the Parser (Section \ref{sec:Parser})
to verify its proper operation with various short garter statements and
functions.  Each such test case specifies a garter language input program and
the string representation of the parse tree which must be produced by the Parser
to pass the test case.  An example is shown in Figure \ref{fig:TestParser}.

\begin{figure}
    \begin{tabular}{|p{4cm}|p{11cm}|}
    \hline
    %\lstset{numbers=left}
    \lstinputlisting{../test/ParserTests/010_Assignment.ga}
    &
    \lstinputlisting{../test/ParserTests/010_Assignment.tree}
    \\ \hline
    \end{tabular}
    \caption{A simple test case for the garter parser.
        In order to pass the test case, the parser must generate the parse tree
        shown in string format on the right as a result of parsing the garter file
        shown on the left.  }
    \label{fig:TestParser}
\end{figure}

Finally, I implemented test cases for the final compiler and interpreter ({\tt
garterc} and {\tt garteri}).  Each such test case consists of (a) a garter
language file demonstrating use of selected language features and usually
containing at least one print statement that prints a value computed by the
program, and (b) the expected output of that program.  The output produced by
the program when either compiled with {\tt garterc} and executed, or interpreted
by {\tt garteri}, must match exactly the expected output in order for the test
case to pass.  An example of one such test case is shown in Figure
\ref{fig:garterc_test}.

\begin{figure}
    \begin{tabular}{|p{9cm}|p{4cm}|}
    \hline
    %\lstset{numbers=left}
    \lstinputlisting{../test/garterc_and_garteri_Tests/060_Prime.ga}
    &
    \lstinputlisting{../test/garterc_and_garteri_Tests/060_Prime.expected_out}
    \\ \hline
    \end{tabular}
    \caption{A sample test case for {\tt garterc} and {\tt garteri}.  The
        testsuite verifies that the program shown to the left generates the exact
        the output shown on the right.  This particular test case (which
        computes and prints all prime numbers below 100, in numerical order) is
        one of the more complicated test cases, and it is run after the correct
    execution of simpler programs has been verified.  }
    \label{fig:garterc_test}
\end{figure}

\section{Other work}

Towards the end of the semester I also investigated the Rust programming
language \cite{rust}, which is a programming language being developed by Mozilla
primarily as a safer alternative to C++.  Rust is a compiled language and uses
LLVM as a back-end, similar to the compiler I implemented for this project.
Rust includes various features that make it easier to write correct, safe
programs compared to C and C++.  For example, in pure Rust there are no ways to
produce undefined behavior, no ways to dereference invalid pointers, no memory
leaks, no race conditions, and no ways to use uninitialized memory.  Null
pointers are replaced by generalized {\tt enum}s, and generalized {\tt match}
statements force all possible cases to be handled.  The Rust compiler is
implemented in Rust itself.  Although I familiarized myself with various parts
of the Rust implementation such as its standard library, I did not have time to
look into the details of how Rust constructs are lowered into LLVM IR. \
However, I did conduct an experiment to see how Rust's {\tt match} statement
(which is like a generalized form of the C {\tt switch} statement) was compiled
into machine code.  It seemed to be done very efficiently, which supports the
practicality of the {\tt match} statement as well as the use of LLVM for easily
including state-of-the-art optimizations in a new compiler.

\bibliographystyle{plain}
\bibliography{refs}

\end{document}
